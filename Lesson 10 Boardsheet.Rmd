---
title: "Lesson 10 $k$-fold Cross-Validation"
author: "Kevin Cummiskey"
date: "February 3, 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Review

Last lesson, we discussed three models to predicts the number of wins in a season.

\begin{align}
Wpct &= \beta_0 + \beta_1 RD + \epsilon \\
Wpct &= \frac{R^2}{R^2 + RA^2} + \epsilon \\
Wpct &= \frac{R^k}{R^k + RA^k} + \epsilon
\end{align}
where $Wpct$ is Win Percentage, $R$ is Runs Scored, $RA$ is Runs Allowed, and $\epsilon$ is the random error. Recall that we fit Model 1 to the 1997-2001 seasons.

When prediction is the goal of a statistical model,  \textit{overfitting} a model to the data is a big concern.  What is \textit{overfitting}?

\vspace{0.5in}

## $k$-fold cross validation

$k$-fold cross validation is one method to assess how well a model will predict on new data.  In this method, we randomly partition the data into $k$ groups.  We set one group (called the \textit{test set}) aside, fit the model on the remaining data (the \textit{training set}), and assess the performance on the test set.  We repeat the process $k$ times with each partition as the test set.

Here is a summary of the steps:

1. Randomly partition the data set into $k$ groups.
2. Set one partition (the test set) aside.
3. Fit the model on the remaining data (the training set).
4. Calculate RMSE.
5. Repeat with each partition as the test set.

The average RMSE of the $k$-folds is a better measure of the model performance.


Let's look at Model 1 using 5-fold cross validation.  

```{r, warning=FALSE, message = FALSE}
library(tidyverse)
library(Lahman)

#get the data
my_teams = Teams %>%
  filter(yearID >= 1997, yearID <= 2001) %>%
  mutate(RD = R - RA,
         Wpct = W/(W + L))

#partition the data into five groups
set.seed(100)
partition = rep(1:5, length.out = 148)
partition = sample(partition, replace = TRUE)
my_teams$partition = partition

# create empty vector to save RMSE for each fold
rmse = c()

# perform cross validation
for(i in 1:5){
  # test set
  test_teams = my_teams %>% 
    filter(partition == i)
  
  #training set
  train_teams = my_teams %>%
    filter(partition != i)

  #fit model
  lin.fit.train = lm(Wpct ~ RD, data = train_teams)
    
  #get predictions and residuals on test set
  test_teams = test_teams %>%
    mutate(Wpct.pred = coef(lin.fit.train)[1] + coef(lin.fit.train)[2]*RD,
           .resid = Wpct - Wpct.pred)
  
  #calculate RMSE
  
  rmse[i] = sqrt(mean(test_teams$.resid^2))
}
```

Let's look at the RMSE for each fold and the average RMSE.

```{r}
round(rmse,4)
round(mean(rmse),4)
```


What do we conclude from this analysis? \vspace{0.5in}


Using the code above as a guide, perform a 5-fold cross validation of the Pythagorean formula with exponent $k$. Report your results and conclusions below.

\vspace{2in}








